{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT PACKAGES\n",
    "import numpy as np\n",
    "import os\n",
    "os.chdir(\"/projects/ml4science/LakeBeD-US/LakeBeD-US-CSE-Benchmark/\")\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pypots.imputation import SAITS\n",
    "import random\n",
    "import torch\n",
    "from Modeling.Utilities import utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>chla_rfu</th>\n",
       "      <th>do</th>\n",
       "      <th>par</th>\n",
       "      <th>phyco</th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006-06-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006-06-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006-06-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006-07-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006-07-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6349</th>\n",
       "      <td>2023-11-15</td>\n",
       "      <td>1.21</td>\n",
       "      <td>9.23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.42</td>\n",
       "      <td>9.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6350</th>\n",
       "      <td>2023-11-16</td>\n",
       "      <td>1.52</td>\n",
       "      <td>9.51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6351</th>\n",
       "      <td>2023-11-17</td>\n",
       "      <td>1.51</td>\n",
       "      <td>9.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>9.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6352</th>\n",
       "      <td>2023-11-18</td>\n",
       "      <td>1.67</td>\n",
       "      <td>9.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6353</th>\n",
       "      <td>2023-11-19</td>\n",
       "      <td>2.05</td>\n",
       "      <td>9.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>9.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6354 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       datetime  chla_rfu    do  par  phyco   temp\n",
       "0    2006-06-28       NaN   NaN  NaN    NaN  22.22\n",
       "1    2006-06-29       NaN   NaN  NaN    NaN  22.34\n",
       "2    2006-06-30       NaN   NaN  NaN    NaN  22.84\n",
       "3    2006-07-01       NaN   NaN  NaN    NaN  22.40\n",
       "4    2006-07-02       NaN   NaN  NaN    NaN  22.40\n",
       "...         ...       ...   ...  ...    ...    ...\n",
       "6349 2023-11-15      1.21  9.23  0.0   0.42   9.33\n",
       "6350 2023-11-16      1.52  9.51  0.0   0.46   9.36\n",
       "6351 2023-11-17      1.51  9.50  0.0   0.45   9.32\n",
       "6352 2023-11-18      1.67  9.58  0.0   0.46   9.15\n",
       "6353 2023-11-19      2.05  9.74  0.0   0.51   9.05\n",
       "\n",
       "[6354 rows x 6 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"Data/2-Data-Harmonization/Mendota_2006_2023.csv\")\n",
    "data[\"datetime\"] = pd.to_datetime(data[\"datetime\"])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback_window = 21\n",
    "training_data, validation_data, testing_data, standardization_parameters = utilities.prepare_data(\n",
    "\tdata = data,\n",
    "\tinput_features = [\"chla_rfu\", \"par\", \"phyco\", \"do\", \"temp\"],\n",
    "\ttarget_features = [\"do\", \"temp\"],\n",
    "\ttraining_fraction = 0.8,\n",
    "\tvalidation_fraction = 0.1,\n",
    "\ttesting_fraction = 0.1,\n",
    "\tlookback_window = lookback_window,\n",
    "\thorizon_window = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 23:36:57 [INFO]: Using the given device: cuda:0\n",
      "2024-11-01 23:36:57 [INFO]: Model files will be saved to Data/3-Data-Imputation/20241101_T233657\n",
      "2024-11-01 23:36:57 [INFO]: Tensorboard file will be saved to Data/3-Data-Imputation/20241101_T233657/tensorboard\n",
      "2024-11-01 23:36:58 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 1,324,719\n"
     ]
    }
   ],
   "source": [
    "saits = SAITS(n_steps = lookback_window,\n",
    "\t\t\t  n_features = 5,\n",
    "\t\t\t  n_layers = 2,\n",
    "\t\t\t  d_model = 256,\n",
    "\t\t\t  n_heads = 4,\n",
    "\t\t\t  d_k = 64, \n",
    "\t\t\t  d_v = 64, \n",
    "\t\t\t  d_ffn = 128, \n",
    "\t\t\t  dropout = 0.1,\n",
    "\t\t\t  epochs = 50,\n",
    "\t\t\t  device = torch.device(\"cuda:0\"),\n",
    "\t\t\t  batch_size = 32,\n",
    "\t\t\t  saving_path = \"Data/3-Data-Imputation\", \n",
    "\t\t\t  verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 23:37:01 [INFO]: Epoch 001 - training loss: 0.7069\n",
      "2024-11-01 23:37:04 [INFO]: Epoch 002 - training loss: 0.4987\n",
      "2024-11-01 23:37:06 [INFO]: Epoch 003 - training loss: 0.4383\n",
      "2024-11-01 23:37:09 [INFO]: Epoch 004 - training loss: 0.4031\n",
      "2024-11-01 23:37:12 [INFO]: Epoch 005 - training loss: 0.3730\n",
      "2024-11-01 23:37:15 [INFO]: Epoch 006 - training loss: 0.3531\n",
      "2024-11-01 23:37:18 [INFO]: Epoch 007 - training loss: 0.3415\n",
      "2024-11-01 23:37:21 [INFO]: Epoch 008 - training loss: 0.3355\n",
      "2024-11-01 23:37:24 [INFO]: Epoch 009 - training loss: 0.3295\n",
      "2024-11-01 23:37:27 [INFO]: Epoch 010 - training loss: 0.3209\n",
      "2024-11-01 23:37:29 [INFO]: Epoch 011 - training loss: 0.3177\n",
      "2024-11-01 23:37:31 [INFO]: Epoch 012 - training loss: 0.3210\n",
      "2024-11-01 23:37:34 [INFO]: Epoch 013 - training loss: 0.3079\n",
      "2024-11-01 23:37:37 [INFO]: Epoch 014 - training loss: 0.3056\n",
      "2024-11-01 23:37:39 [INFO]: Epoch 015 - training loss: 0.3006\n",
      "2024-11-01 23:37:42 [INFO]: Epoch 016 - training loss: 0.2999\n",
      "2024-11-01 23:37:45 [INFO]: Epoch 017 - training loss: 0.2988\n",
      "2024-11-01 23:37:48 [INFO]: Epoch 018 - training loss: 0.2937\n",
      "2024-11-01 23:37:51 [INFO]: Epoch 019 - training loss: 0.2909\n",
      "2024-11-01 23:37:53 [INFO]: Epoch 020 - training loss: 0.2925\n",
      "2024-11-01 23:37:56 [INFO]: Epoch 021 - training loss: 0.2902\n",
      "2024-11-01 23:37:58 [INFO]: Epoch 022 - training loss: 0.2896\n",
      "2024-11-01 23:38:01 [INFO]: Epoch 023 - training loss: 0.2926\n",
      "2024-11-01 23:38:03 [INFO]: Epoch 024 - training loss: 0.2887\n",
      "2024-11-01 23:38:06 [INFO]: Epoch 025 - training loss: 0.2858\n",
      "2024-11-01 23:38:09 [INFO]: Epoch 026 - training loss: 0.2839\n",
      "2024-11-01 23:38:12 [INFO]: Epoch 027 - training loss: 0.2848\n",
      "2024-11-01 23:38:15 [INFO]: Epoch 028 - training loss: 0.2815\n",
      "2024-11-01 23:38:17 [INFO]: Epoch 029 - training loss: 0.2812\n",
      "2024-11-01 23:38:20 [INFO]: Epoch 030 - training loss: 0.2818\n",
      "2024-11-01 23:38:23 [INFO]: Epoch 031 - training loss: 0.2782\n",
      "2024-11-01 23:38:26 [INFO]: Epoch 032 - training loss: 0.2780\n",
      "2024-11-01 23:38:28 [INFO]: Epoch 033 - training loss: 0.2815\n",
      "2024-11-01 23:38:31 [INFO]: Epoch 034 - training loss: 0.2828\n",
      "2024-11-01 23:38:33 [INFO]: Epoch 035 - training loss: 0.2804\n",
      "2024-11-01 23:38:36 [INFO]: Epoch 036 - training loss: 0.2767\n",
      "2024-11-01 23:38:39 [INFO]: Epoch 037 - training loss: 0.2787\n",
      "2024-11-01 23:38:42 [INFO]: Epoch 038 - training loss: 0.2727\n",
      "2024-11-01 23:38:45 [INFO]: Epoch 039 - training loss: 0.2741\n",
      "2024-11-01 23:38:48 [INFO]: Epoch 040 - training loss: 0.2721\n",
      "2024-11-01 23:38:51 [INFO]: Epoch 041 - training loss: 0.2707\n",
      "2024-11-01 23:38:54 [INFO]: Epoch 042 - training loss: 0.2723\n",
      "2024-11-01 23:38:56 [INFO]: Epoch 043 - training loss: 0.2740\n",
      "2024-11-01 23:38:59 [INFO]: Epoch 044 - training loss: 0.2724\n",
      "2024-11-01 23:39:01 [INFO]: Epoch 045 - training loss: 0.2714\n",
      "2024-11-01 23:39:04 [INFO]: Epoch 046 - training loss: 0.2721\n",
      "2024-11-01 23:39:06 [INFO]: Epoch 047 - training loss: 0.2721\n",
      "2024-11-01 23:39:09 [INFO]: Epoch 048 - training loss: 0.2737\n",
      "2024-11-01 23:39:12 [INFO]: Epoch 049 - training loss: 0.2693\n",
      "2024-11-01 23:39:15 [INFO]: Epoch 050 - training loss: 0.2686\n",
      "2024-11-01 23:39:15 [INFO]: Finished training. The best model is from epoch#50.\n",
      "2024-11-01 23:39:15 [INFO]: Saved the model to Data/3-Data-Imputation/20241101_T233657/SAITS.pypots\n"
     ]
    }
   ],
   "source": [
    "saits.fit({'X': training_data[\"windowed_data\"]['X']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[\"windowed_data_imputed\"]['X'] = saits.impute({'X': training_data[\"windowed_data\"]['X']})\n",
    "validation_data[\"windowed_data_imputed\"]['X'] = saits.impute({'X': validation_data[\"windowed_data\"]['X']})\n",
    "testing_data[\"windowed_data_imputed\"]['X'] = saits.impute({'X': testing_data[\"windowed_data\"]['X']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data/3-Data-Imputation/Training_Data.pickle\", \"wb\") as f:\n",
    "\tpickle.dump(training_data, f)\n",
    "\n",
    "with open(\"Data/3-Data-Imputation/Validation_Data.pickle\", \"wb\") as f:\n",
    "\tpickle.dump(validation_data, f)\n",
    "\n",
    "with open(\"Data/3-Data-Imputation/Testing_Data.pickle\", \"wb\") as f:\n",
    "\tpickle.dump(testing_data, f)\n",
    "\n",
    "with open(\"Data/3-Data-Imputation/Standardization_Parameters.pickle\", \"wb\") as f:\n",
    "\tpickle.dump(standardization_parameters, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAITS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
